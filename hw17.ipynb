{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RRtdwG3HzInG",
        "5MpPSmX4zGuG",
        "8sph3jiZISkH",
        "VTBc5vVdzNJe",
        "qWx5LnjzzXQm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpoDip8bIGe"
      },
      "source": [
        "# Homeworks\n",
        "1. Find the answer to the question raised in the lab1\n",
        "\n",
        "    Some helpful resources:\n",
        "- DeepWalk: https://arxiv.org/pdf/1403.6652.pdf\n",
        "- Word2vec: https://arxiv.org/pdf/1301.3781.pdf\n",
        "- Repository Github of Word2vec at [this link](https://github.com/RaRe-Technologies/gensim)\n",
        "2. Implement a simple word2vec algorithm for the DeepWalk (Attributes for each node should be created).\n",
        "3. Use some libraries to solve a real problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWpAGFkTy9LW"
      },
      "source": [
        "# Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKZjx1qdgl5u"
      },
      "source": [
        "## Implement Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Gr0JI5qUUG"
      },
      "source": [
        "### Download data and install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AaWu-oYRTPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b310d54c-a2ef-44da-f1c1-76814e8ec81d"
      },
      "source": [
        "!pip install karateclub==1.2.0\n",
        "!pip install umap-learn"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: karateclub==1.2.0 in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.23.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (3.4)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (4.66.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (0.16)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.13.0)\n",
            "Requirement already satisfied: pygsp in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (0.5.1)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (4.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (2.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->karateclub==1.2.0) (7.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->karateclub==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->karateclub==1.2.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->karateclub==1.2.0) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->karateclub==1.2.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->karateclub==1.2.0) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->karateclub==1.2.0) (1.16.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe8SGQmHNH1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aac32ab-96f4-4aba-a6a0-2259b5bf8708"
      },
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install scipy==1.13.0"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scipy==1.13.0 in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy==1.13.0) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naI1TUJORTPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5f0e4a-9c4f-4967-956a-8a0eb03041b8"
      },
      "source": [
        "!gdown --id \"1RmrHId0d-uY7kJCSgCtNYbwYfp4Oum3c&export=download\"\n",
        "!unrar x -Y \"/content/lab3.rar\" -d \"/content/\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RmrHId0d-uY7kJCSgCtNYbwYfp4Oum3c&export=download\n",
            "To: /content/lab3.rar\n",
            "100% 1.54M/1.54M [00:00<00:00, 43.8MB/s]\n",
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/lab3.rar\n",
            "\n",
            "Extracting  /content/lab3_attributes.csv                                 \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/facebook_features.json                              \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/facebook_target.csv                                 \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lab3_edgelist.txt                                   \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/facebook_edges.csv                                  \b\b\b\b100%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRtdwG3HzInG"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeFtXGup5clN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4w2axVr6kRD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hv66dDbu6xL9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7MLBGJpBjv-"
      },
      "source": [
        "\n",
        "# Task 1\n",
        "import networkx as nx\n",
        "from joblib import Parallel, delayed\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Task 2\n",
        "import json\n",
        "import umap\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from karateclub.utils.walker import RandomWalker\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MpPSmX4zGuG"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wM_CQ-MCMCu"
      },
      "source": [
        "def partition_num(num, workers):\n",
        "    if num % workers == 0:\n",
        "        return [num//workers]*workers\n",
        "    else:\n",
        "        return [num//workers]*workers + [num % workers]\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "def get_attributes_of_node(node_paths):\n",
        "  node_paths_attributes = []\n",
        "  # Get attribute (word) for each node\n",
        "  df_attr = pd.read_csv(\"lab3_attributes.csv\").astype(str)\n",
        "  dict_attr = {}\n",
        "  for i in range(len(df_attr)):\n",
        "    dict_attr[df_attr.iloc[i, 0]] = df_attr.iloc[i, 1]\n",
        "  for path in node_paths:\n",
        "    for index, node in enumerate(path):\n",
        "      path[index] = dict_attr[node]\n",
        "    node_paths_attributes.append(path)\n",
        "  return node_paths_attributes\n",
        "\n",
        "def preprocessing(sentences):\n",
        "    training_data = []\n",
        "    for sentence in sentences:\n",
        "        x = [word for word in sentence]\n",
        "        training_data.append(x)\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "\n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "\n",
        "    return w2v.X_train,w2v.y_train"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sph3jiZISkH"
      },
      "source": [
        "### TO DO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class word2vec:\n",
        "    def __init__(self, window_size=5, epochs=100, hidden_size=100, learning_rate=0.001):\n",
        "        self.window_size = window_size\n",
        "        self.epochs = epochs\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.X_train = []\n",
        "        self.y_train = []\n",
        "        self.vocab = {}\n",
        "        self.V = None\n",
        "        self.words = None\n",
        "        self.w1 = None\n",
        "        self.w2 = None\n",
        "\n",
        "    def initialize(self, V, data):\n",
        "        self.V = V\n",
        "        self.words = data\n",
        "        self.vocab = {word: index for index, word in enumerate(data)}\n",
        "        self.w1 = np.random.uniform(-0.8, 0.8, (self.V, self.hidden_size))\n",
        "        self.w2 = np.random.uniform(-0.8, 0.8, (self.hidden_size, self.V))\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = 0\n",
        "            for i in range(len(self.X_train)):\n",
        "                # Forward propagation\n",
        "                x = np.array(self.X_train[i])\n",
        "                y_true = np.array(self.y_train[i])\n",
        "\n",
        "                h = np.dot(x, self.w1)\n",
        "                u = np.dot(h, self.w2)\n",
        "                y_pred = softmax(u)\n",
        "\n",
        "                # Calculate loss (cross-entropy)\n",
        "                loss_example = -np.sum(y_true * np.log(y_pred + 1e-8))\n",
        "                loss += loss_example\n",
        "\n",
        "                # Backpropagation\n",
        "                error = y_pred - y_true\n",
        "                dw2 = np.outer(h, error)  # Gradient for w2\n",
        "                dw1 = np.outer(x, np.dot(self.w2, error))  # Gradient for w1\n",
        "\n",
        "                # Update weights\n",
        "                self.w1 -= self.learning_rate * dw1\n",
        "                self.w2 -= self.learning_rate * dw2\n",
        "\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f'Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "    def predict(self, word, number_of_predictions):\n",
        "        if word not in self.vocab:\n",
        "            print(f\"Word '{word}' not in vocabulary.\")\n",
        "            return []\n",
        "\n",
        "        index = self.vocab[word]\n",
        "        embedding = self.w1[index]  # Embedding vector for the input word\n",
        "        similarities = {}\n",
        "        for i in range(self.V):\n",
        "            if i != index:\n",
        "                other_embedding = self.w1[i]\n",
        "                # Compute cosine similarity\n",
        "                cosine_sim = np.dot(embedding, other_embedding) / (np.linalg.norm(embedding) * np.linalg.norm(other_embedding) + 1e-9)\n",
        "                similarities[self.words[i]] = cosine_sim\n",
        "\n",
        "        # Sort by similarity\n",
        "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_predictions = [word for word, _ in sorted_similarities[:number_of_predictions]]\n",
        "        return top_predictions\n"
      ],
      "metadata": {
        "id": "Izbr3OWF4cHP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTBc5vVdzNJe"
      },
      "source": [
        "### DeepWalk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjIDrqVB-vtm"
      },
      "source": [
        "class RandomWalker:\n",
        "  def __init__(self, G, num_walks, walk_length):\n",
        "      \"\"\"\n",
        "      :param G: Graph\n",
        "      :param num_walks: a number of walks\n",
        "      :param walk_length: Length of a walk. Each walk is considered as a sentence\n",
        "      \"\"\"\n",
        "      self.G = G\n",
        "      self.num_walks = num_walks\n",
        "      self.walk_length = walk_length\n",
        "\n",
        "\n",
        "  def deepwalk_walk(self, start_node):\n",
        "      \"\"\"\n",
        "      :param start_node: Starting node of a walk\n",
        "      \"\"\"\n",
        "      walk = [start_node]\n",
        "      while len(walk) < self.walk_length:\n",
        "          cur = walk[-1]\n",
        "          # Check if having any neighbors at the current node\n",
        "          cur_nbrs = list(self.G.neighbors(cur))\n",
        "          if len(cur_nbrs) > 0:\n",
        "              # Random walk with the probability of 1/d(v^t). d(v^t) is the node degree\n",
        "              walk.append(random.choice(cur_nbrs))\n",
        "          else:\n",
        "              break\n",
        "      return walk\n",
        "\n",
        "\n",
        "  def simulate_walks(self, workers=1, verbose=0):\n",
        "      \"\"\"\n",
        "      :param workers: a number of workers running in parallel processing\n",
        "      :param verbose: progress bar\n",
        "      \"\"\"\n",
        "      G = self.G\n",
        "      nodes = list(G.nodes())\n",
        "      results = Parallel(n_jobs=workers, verbose=verbose)(\n",
        "          delayed(self._simulate_walks)(nodes) for num in\n",
        "          partition_num(self.num_walks, workers))\n",
        "      walks = list(itertools.chain(*results))\n",
        "      return walks\n",
        "\n",
        "\n",
        "  # INFORMATION EXTRACTOR\n",
        "  def _simulate_walks(self, nodes):\n",
        "      walks = []\n",
        "      # Iterate all walks per vertex\n",
        "      for _ in range(self.num_walks):\n",
        "          random.shuffle(nodes)\n",
        "          # Iterate all nodes in a walk\n",
        "          for v in nodes:\n",
        "            walks.append(self.deepwalk_walk(start_node=v))\n",
        "      return walks"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYUUSiDD-VdL"
      },
      "source": [
        "class DeepWalk:\n",
        "    def __init__(self, graph, walk_length, num_walks, workers=1):\n",
        "\n",
        "        self.graph = graph\n",
        "        self.w2v_model = None\n",
        "        self._embeddings = {}\n",
        "\n",
        "        self.walker = RandomWalker(graph, num_walks=num_walks, walk_length=walk_length)\n",
        "        self.walks = self.walker.simulate_walks(workers=workers, verbose=1)\n",
        "        self.sentences = get_attributes_of_node(self.walks)\n",
        "\n",
        "\n",
        "    def train(self, window_size=5, epochs=100):\n",
        "        print(\"Learning embedding vectors...\")\n",
        "        training_data = preprocessing(self.sentences)\n",
        "        w2v = word2vec(window_size, epochs)\n",
        "        prepare_data_for_training(training_data, w2v)\n",
        "        w2v.train()\n",
        "        print(\"Learning embedding vectors done!\")\n",
        "        self.w2v_model = w2v\n",
        "\n",
        "\n",
        "    def test(self, word):\n",
        "        print(self.w2v_model.predict(word,3))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJBjYaCw6zBr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWx5LnjzzXQm"
      },
      "source": [
        "### Run graph embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtwVs_UrAFqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e70a608-3c9b-4223-d91e-0b3a07337bf6"
      },
      "source": [
        "G = nx.read_edgelist('lab3_edgelist.txt',create_using=nx.DiGraph(),nodetype=None,data=[('weight',int)])# Read graph\n",
        "model = DeepWalk(G, walk_length=3, num_walks=10, workers=1)#init model\n",
        "model.train(window_size=5)# train model"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning embedding vectors...\n",
            "Epoch 1/100, Loss: 5984.9095\n",
            "Epoch 10/100, Loss: 3792.8578\n",
            "Epoch 20/100, Loss: 3897.2405\n",
            "Epoch 30/100, Loss: 4500.7118\n",
            "Epoch 40/100, Loss: 5405.7888\n",
            "Epoch 50/100, Loss: 6144.6849\n",
            "Epoch 60/100, Loss: 6634.5734\n",
            "Epoch 70/100, Loss: 7128.0932\n",
            "Epoch 80/100, Loss: 7814.6353\n",
            "Epoch 90/100, Loss: 9690.5579\n",
            "Epoch 100/100, Loss: 13173.0410\n",
            "Learning embedding vectors done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKyjIZ5T2e9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bffc9985-caf1-41bd-fd32-7d34d6430900"
      },
      "source": [
        "print(model.sentences)\n",
        "model.test(\"to\")\n",
        "model.test(\"this\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['happy', 'wish', 'learned'], ['am', 'that', 'this'], ['so', 'that', 'you'], ['to', 'happy', 'wish'], ['this', 'so', 'that'], ['wish', 'all', 'all'], ['the', 'all', 'you'], ['!', 'am', 'you'], ['that', 'you', 'to'], ['.', 'so', '!'], ['something', 'learned', '!'], ['I', 'to', '!'], ['you', 'to', 'lab'], ['learned', 'this', 'so'], ['in', 'all', 'am'], ['lab', 'happy', 'wish'], ['.', '.', 'happy'], ['this', 'happy', 'that'], ['I', 'happy', 'wish'], ['lab', '.', 'happy'], ['best', 'best', 'best'], ['all', 'all', 'all'], ['you', 'I', 'this'], ['.', '.', 'happy'], ['!', 'am', 'this'], ['this', 'this', 'I'], ['to', 'you', 'you'], ['so', 'I', 'you'], ['wish', 'all', 'am'], ['learned', 'this', 'to'], ['I', 'to', 'you'], ['best', 'I', 'am'], ['I', 'happy', 'wish'], ['all', 'this', 'so'], ['you', 'to', '!'], ['that', 'this', 'so'], ['am', 'am', 'you'], ['lab', '.', 'happy'], ['happy', 'wish', 'learned'], ['you', 'I', 'this'], ['lab', 'am', 'am'], ['something', 'wish', 'that'], ['.', 'so', 'that'], ['this', 'happy', 'wish'], ['in', 'to', 'you'], ['the', 'all', 'you'], ['so', 'I', 'to'], ['lab', '.', 'so'], ['.', '.', 'happy'], ['lab', 'happy', 'wish'], ['this', 'this', 'this'], ['something', 'you', 'to'], ['I', 'happy', 'that'], ['wish', 'that', 'this'], ['am', 'you', 'you'], ['learned', '!', 'am'], ['to', 'lab', 'happy'], ['you', 'you', 'to'], ['happy', 'that', 'you'], ['best', 'you', 'to'], ['!', 'am', 'am'], ['you', 'I', 'happy'], ['I', 'to', 'happy'], ['that', 'this', 'to'], ['.', 'happy', 'that'], ['in', 'to', 'lab'], ['the', 'all', 'so'], ['all', 'you', 'this'], ['this', 'so', 'that'], ['learned', 'I', 'am'], ['that', 'this', 'to'], ['lab', '.', 'happy'], ['lab', 'am', 'you'], ['this', 'so', 'that'], ['you', 'this', 'so'], ['wish', 'you', 'to'], ['you', 'to', '!'], ['.', '.', '.'], ['something', 'wish', 'that'], ['I', 'this', 'so'], ['I', 'am', 'am'], ['.', 'happy', 'that'], ['happy', 'that', 'you'], ['in', 'to', '!'], ['the', 'I', 'to'], ['all', 'you', 'this'], ['this', 'this', 'happy'], ['!', 'I', 'happy'], ['to', '!', 'to'], ['best', 'to', '!'], ['am', 'am', 'this'], ['so', '!', 'to'], ['!', 'to', '!'], ['I', 'happy', 'wish'], ['am', 'you', 'to'], ['best', 'best', 'best'], ['this', 'happy', 'wish'], ['learned', '!', 'so'], ['I', 'to', 'happy'], ['.', 'so', 'lab'], ['.', '.', 'so'], ['the', 'all', 'am'], ['lab', '.', 'so'], ['something', 'you', 'to'], ['this', 'I', 'to'], ['so', 'that', 'you'], ['wish', 'you', 'you'], ['to', '!', 'am'], ['in', 'am', 'that'], ['lab', 'am', 'that'], ['happy', 'wish', 'you'], ['all', 'you', 'that'], ['you', 'I', 'you'], ['that', 'you', 'to'], ['you', 'you', 'to'], ['that', 'this', 'I'], ['the', 'all', 'am'], ['.', 'happy', 'wish'], ['you', 'that', 'this'], ['lab', 'am', 'this'], ['you', 'you', 'you'], ['.', '.', 'so'], ['so', 'lab', 'am'], ['this', 'happy', 'that'], ['am', 'that', 'this'], ['something', 'happy', 'wish'], ['all', 'to', 'lab'], ['learned', 'I', 'you'], ['to', 'happy', 'wish'], ['lab', '.', 'so'], ['happy', 'that', 'you'], ['I', 'happy', 'wish'], ['in', 'am', 'am'], ['I', 'am', 'that'], ['this', 'so', 'I'], ['best', 'best', 'you'], ['!', 'am', 'this'], ['wish', 'learned', 'this'], ['you', '.', 'so'], ['this', 'happy', 'wish'], ['in', 'to', 'happy'], ['wish', 'that', 'you'], ['that', 'you', 'to'], ['lab', '.', 'so'], ['lab', 'am', 'you'], ['the', 'so', 'that'], ['to', '!', 'so'], ['you', 'you', 'to'], ['am', 'that', 'you'], ['this', 'to', 'happy'], ['I', 'am', 'am'], ['all', 'to', 'lab'], ['I', 'happy', 'wish'], ['learned', 'I', 'you'], ['happy', 'that', 'you'], ['.', '.', 'happy'], ['so', 'lab', 'happy'], ['!', 'so', 'lab'], ['something', 'wish', 'all'], ['best', 'you', 'to'], ['.', 'so', '!'], ['all', 'am', 'this'], ['you', 'to', 'lab'], ['something', 'learned', 'I'], ['lab', '.', 'happy'], ['this', 'this', 'to'], ['you', 'I', 'happy'], ['learned', 'I', 'am'], ['.', '.', '.'], ['lab', 'am', 'am'], ['!', 'so', 'that'], ['in', 'all', 'to'], ['wish', 'learned', 'this'], ['happy', 'that', 'you'], ['I', 'to', '!'], ['best', 'to', 'happy'], ['am', 'this', 'I'], ['so', 'I', 'am'], ['the', 'I', 'to'], ['.', 'happy', 'wish'], ['that', 'you', 'to'], ['this', 'I', 'happy'], ['to', 'you', 'to'], ['I', 'you', 'you'], ['all', 'so', '!'], ['to', 'lab', 'am'], ['that', 'this', 'so'], ['the', 'I', 'to'], ['.', '.', 'so'], ['learned', 'this', 'I'], ['.', 'happy', 'wish'], ['wish', 'you', 'you'], ['something', 'learned', 'this'], ['this', 'to', 'lab'], ['!', 'am', 'this'], ['lab', '.', 'so'], ['so', '!', 'I'], ['you', 'to', 'you'], ['you', '.', 'so'], ['lab', 'happy', 'wish'], ['I', 'you', 'you'], ['am', 'this', 'to'], ['in', 'to', 'you'], ['best', 'you', 'to'], ['this', 'this', 'so'], ['I', 'to', 'happy'], ['happy', 'that', 'you'], ['lab', '.', 'so'], ['that', 'this', 'this'], ['.', 'so', '!'], ['all', 'all', 'am'], ['best', 'best', 'I'], ['I', 'you', 'to'], ['am', 'you', 'to'], ['the', 'so', '!'], ['in', 'to', 'happy'], ['you', 'you', 'to'], ['wish', 'learned', '!'], ['!', 'am', 'this'], ['to', 'you', 'you'], ['.', '.', '.'], ['I', 'to', 'you'], ['this', 'to', 'you'], ['you', 'I', 'you'], ['happy', 'that', 'this'], ['so', '!', 'am'], ['lab', 'am', 'this'], ['this', 'this', 'so'], ['something', 'you', 'to'], ['learned', 'this', 'to']]\n",
            "['you', 'am', 'this']\n",
            "['you', 'I', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snCfP6XKiBcW"
      },
      "source": [
        "## TO DO: Solve a real problem using some libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3OKBtsrERdI"
      },
      "source": [
        "Goal: When we have a large graph dataset like the Facebook dataset below, we want to classify which company (node) will likely belong to a type of page. If we categorize well, we could apply marketing strategies in a domain on a company that we are surveying.\n",
        "Therefore, our task is to learn a model which can classify a company using related features.\n",
        "\n",
        "\n",
        "1. Analyze and visualize the dataset Facebook downloaded in [this website](https://snap.stanford.edu/data/facebook-large-page-page-network.html).\n",
        "2. Use DeepWalk to embed the graph\n",
        "3. Train a classifier to do the node classification task using the embedding graph from step 2.\n",
        "\n",
        "You can do many things with the data. I recommend that you could try many tasks with this data, not only the classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdTHjzzg1OGo"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy0GThOYF2HQ"
      },
      "source": [
        "edges_path = 'facebook_edges.csv'\n",
        "targets_path = 'facebook_target.csv'\n",
        "features_path = 'facebook_features.json'\n",
        "edges = pd.read_csv('facebook_edges.csv')\n",
        "targets = pd.read_csv('facebook_target.csv')\n",
        "with open('facebook_features.json', 'r') as f:\n",
        "    features = json.load(f)\n",
        "print(features)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIjmgPL11URP"
      },
      "source": [
        "### Visualize datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOSNdGIeGwo_"
      },
      "source": [
        "Create a graph. If you want to use smaller graph, please try to create one. It will be lighter when running the code."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6G3NhTK7xft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrHJTDiC1bUn"
      },
      "source": [
        "### Embedding graph using DeepWalk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyw2GTlRD-kX"
      },
      "source": [
        "Embedding graph using DeepWalk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaIerx0GQlTX"
      },
      "source": [
        "### Train a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbVeCfFmG70H"
      },
      "source": [
        "Train a classifer from the embedding graph to the target. Here we use the Random Forest classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITdR7O6VFtip"
      },
      "source": [
        "# THANK YOU\n",
        "Please dive more into the codes and papers if you are interested.\n",
        "\n",
        "Thank you for joining all the labs."
      ]
    }
  ]
}